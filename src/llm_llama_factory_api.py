import os
import yaml
import subprocess
import sys
from pathlib import Path

import os
import logging
import sys
from logger import init_logger

base_dir = os.path.split(os.path.realpath(__file__))[0]
program = os.path.basename(sys.argv[0])
logger = logging.getLogger(program)
if not logger.handlers:
    logger = init_logger('../data/log')


def create_inference_yaml_config(dataset_dir='./data/dataset',
                                 eval_dataset='alpaca_zh_demo',
                                 output_dir='./data/saves/Qwen3-8B/lora/predict',
                                 model_name_or_path='./data/model/Qwen3-8B',
                                 adapter_name_or_path=None,
                                 yaml_file_path='./data/train_lora/predict.yaml',
                                 test_max_samples=100000,
                                 per_device_eval_batch_size=16,
                                 temperature=0.2):
    pass
    # å®šä¹‰é…ç½®æ•°æ®
    config = {
        'model_name_or_path': model_name_or_path,
        'trust_remote_code': True,
        'adapter_name_or_path': adapter_name_or_path,
        # method
        'stage': 'sft',
        'do_predict': True,
        'finetuning_type': 'lora',

        'dataset_dir': dataset_dir,
        'eval_dataset': eval_dataset,
        'template': 'qwen',
        'cutoff_len': 1024,
        'max_samples': test_max_samples,
        'overwrite_cache': True,
        'preprocessing_num_workers': 16,
        'dataloader_num_workers': 4,
        'per_device_eval_batch_size': per_device_eval_batch_size,
        'temperature': temperature,

        'output_dir': output_dir,
        'overwrite_output_dir': True,
        'predict_with_generate': True

    }

    # ç¡®ä¿/data/ç›®å½•å­˜åœ¨
    os.makedirs('./data', exist_ok=True)

    try:
        with open(yaml_file_path, 'w', encoding='utf-8') as f:
            # æ·»åŠ æ³¨é‡Šåˆ†ç»„
            f.write("### model\n")
            if config['adapter_name_or_path'] is not None:
                yaml.dump({
                    'model_name_or_path': config['model_name_or_path'],
                    'trust_remote_code': config['trust_remote_code'],
                    'adapter_name_or_path': config['adapter_name_or_path']
                }, f, default_flow_style=False, allow_unicode=True)

                f.write("\n### method\n")
                yaml.dump({
                    'stage': config['stage'],
                    'do_predict': config['do_predict'],
                    'finetuning_type': config['finetuning_type']
                }, f, default_flow_style=False, allow_unicode=True)
            else:
                yaml.dump({
                    'model_name_or_path': config['model_name_or_path'],
                    'trust_remote_code': config['trust_remote_code'],
                }, f, default_flow_style=False, allow_unicode=True)

                f.write("\n### method\n")
                yaml.dump({
                    'stage': config['stage'],
                    'do_predict': config['do_predict'],
                }, f, default_flow_style=False, allow_unicode=True)


            f.write("\n### dataset\n")
            yaml.dump({
                'dataset_dir': config['dataset_dir'],
                'eval_dataset': config['eval_dataset'],
                'template': config['template'],
                'cutoff_len': config['cutoff_len'],
                'max_samples': config['max_samples'],
                'overwrite_cache': config['overwrite_cache'],
                'preprocessing_num_workers': config['preprocessing_num_workers'],
                'dataloader_num_workers': config['dataloader_num_workers'],
                'per_device_eval_batch_size': config['per_device_eval_batch_size']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n### output\n")
            yaml.dump({
                'output_dir': config['output_dir'],
                'overwrite_output_dir': config['overwrite_output_dir'],
                'predict_with_generate': config['predict_with_generate']
            }, f, default_flow_style=False, allow_unicode=True)

        logger.info(f"âœ… æˆåŠŸåˆ›å»ºé…ç½®æ–‡ä»¶: {yaml_file_path}")
        return yaml_file_path

    except Exception as e:
        logger.info(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def create_train_yaml_config(dataset_dir='../data/dataset',  # '/mnt/d/project-hxd/pprs/data',
                             dataset='alpaca_zh_demo',
                             output_dir='./data/saves/Qwen3-8B/lora/sft',
                             # '/home/dell/LLaMA-Factory/saves/Qwen3-4B/lora/sft',
                             model_name_or_path='../data/llm/model/Qwen3-8B',  # '/home/dell/LLaMA-Factory/models/Qwen3-4B',
                             yaml_file_path='./data/train_lora/train.yaml',
                             train_max_samples=500000,
                             num_train_epochs=3):
    """
    dataset_dir: å­˜æ”¾dataset_info.jsonæ–‡ä»¶å’Œæ•°æ®é›†çš„åœ°æ–¹
    datasetï¼šdataset_info.jsonä¸­æ•°æ®é›†çš„key
    output_dir: å­˜æ”¾å¾®è°ƒçš„å‚æ•°çš„åœ°æ–¹
    model_name_or_path: å­˜æ”¾åŸå§‹å¤§æ¨¡å‹çš„åœ°æ–¹
    yaml_file_path: å­˜æ”¾yamlæ–‡ä»¶çš„è·¯å¾„
    : return
    è¿”å› test.yamlé…ç½®æ–‡ä»¶
    """
    # å®šä¹‰é…ç½®æ•°æ®
    config = {
        # model
        'model_name_or_path': model_name_or_path,
        'trust_remote_code': True,

        # method
        'stage': 'sft',
        'do_train': True,
        'finetuning_type': 'lora',
        'lora_rank': 8,
        'lora_target': 'all',

        # dataset (æ ¹æ®è¦æ±‚ä¿®æ”¹)
        'dataset_dir': dataset_dir,
        'dataset': dataset,
        'template': 'qwen',
        'cutoff_len': 2048,
        'max_samples': train_max_samples,
        'overwrite_cache': True,
        'preprocessing_num_workers': 16,
        'dataloader_num_workers': 4,

        # output (æ ¹æ®è¦æ±‚ä¿®æ”¹)
        'output_dir': output_dir,
        'logging_steps': 10,
        'save_steps': 500,
        'plot_loss': True,
        'overwrite_output_dir': True,
        'save_only_model': False,
        'report_to': 'none',

        # train
        'per_device_train_batch_size': 1,
        'gradient_accumulation_steps': 8,
        'learning_rate': 1.0e-4,
        'num_train_epochs': num_train_epochs,
        'lr_scheduler_type': 'cosine',
        'warmup_ratio': 0.1,
        'bf16': True,
        'ddp_timeout': 180000000,
        'resume_from_checkpoint': None,

        # eval (æ³¨é‡Šæ‰çš„éƒ¨åˆ†ï¼Œè¿™é‡Œç”¨Noneè¡¨ç¤ºæˆ–è€…ä¸åŒ…å«)
        # 'eval_dataset': dataset,
        # 'val_size': 0.1,
        # 'per_device_eval_batch_size': 1,
        # 'eval_strategy': 'steps',
        # 'eval_steps': 500
    }

    # ç¡®ä¿/data/ç›®å½•å­˜åœ¨
    os.makedirs('./data', exist_ok=True)

    try:
        with open(yaml_file_path, 'w', encoding='utf-8') as f:
            # æ·»åŠ æ³¨é‡Šåˆ†ç»„
            f.write("### model\n")
            yaml.dump({
                'model_name_or_path': config['model_name_or_path'],
                'trust_remote_code': config['trust_remote_code']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n### method\n")
            yaml.dump({
                'stage': config['stage'],
                'do_train': config['do_train'],
                'finetuning_type': config['finetuning_type'],
                'lora_rank': config['lora_rank'],
                'lora_target': config['lora_target']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n### dataset\n")
            yaml.dump({
                'dataset_dir': config['dataset_dir'],
                'dataset': config['dataset'],
                'template': config['template'],
                'cutoff_len': config['cutoff_len'],
                'max_samples': config['max_samples'],
                'overwrite_cache': config['overwrite_cache'],
                'preprocessing_num_workers': config['preprocessing_num_workers'],
                'dataloader_num_workers': config['dataloader_num_workers']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n### output\n")
            yaml.dump({
                'output_dir': config['output_dir'],
                'logging_steps': config['logging_steps'],
                'save_steps': config['save_steps'],
                'plot_loss': config['plot_loss'],
                'overwrite_output_dir': config['overwrite_output_dir'],
                'save_only_model': config['save_only_model'],
                'report_to': config['report_to']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n# choices: [none, wandb, tensorboard, swanlab, mlflow]\n")

            f.write("\n### train\n")
            yaml.dump({
                'per_device_train_batch_size': config['per_device_train_batch_size'],
                'gradient_accumulation_steps': config['gradient_accumulation_steps'],
                'learning_rate': config['learning_rate'],
                'num_train_epochs': config['num_train_epochs'],
                'lr_scheduler_type': config['lr_scheduler_type'],
                'warmup_ratio': config['warmup_ratio'],
                'bf16': config['bf16'],
                'ddp_timeout': config['ddp_timeout'],
                'resume_from_checkpoint': config['resume_from_checkpoint']
            }, f, default_flow_style=False, allow_unicode=True)

            f.write("\n### eval\n")
            f.write("# eval_dataset: alpaca_en_demo\n")
            f.write("# val_size: 0.1\n")
            f.write("# per_device_eval_batch_size: 1\n")
            f.write("# eval_strategy: steps\n")
            f.write("# eval_steps: 500\n")

        logger.info(f"âœ… æˆåŠŸåˆ›å»ºé…ç½®æ–‡ä»¶: {yaml_file_path}")
        return yaml_file_path

    except Exception as e:
        logger.info(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


def run_llamafactory_train(config_path):
    """
    ä½¿ç”¨llamafactory-cli trainè¿è¡Œè®­ç»ƒ
    """
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(config_path):
            logger.info(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False

        # æ„å»ºå‘½ä»¤
        cmd = ['llamafactory-cli', 'train', config_path]

        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œè®­ç»ƒå‘½ä»¤: {' '.join(cmd)}")
        logger.info("=" * 50)

        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            capture_output=False,  # è®©è¾“å‡ºç›´æ¥æ˜¾ç¤ºåœ¨ç»ˆç«¯
            text=True,
            check=False  # ä¸è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸ï¼Œæ‰‹åŠ¨æ£€æŸ¥è¿”å›ç 
        )

        if result.returncode == 0:
            logger.info("=" * 50)
            logger.info("âœ… è®­ç»ƒå‘½ä»¤æ‰§è¡ŒæˆåŠŸ!")
            return True
        else:
            logger.info("=" * 50)
            logger.info(f"âŒ è®­ç»ƒå‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            return False

    except FileNotFoundError:
        logger.info("âŒ æ‰¾ä¸åˆ° llamafactory-cli å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… LLaMA-Factory")
        logger.info("   å¯ä»¥å°è¯•: pip install llamafactory")
        return False
    except Exception as e:
        logger.info(f"âŒ æ‰§è¡Œè®­ç»ƒæ—¶å‡ºé”™: {e}")
        return False


def llm_train(dataset, output_dir, yaml_file_path, dataset_dir, model_name_or_path, train_max_samples=500000, num_train_epochs=3):
    """
    ä¸»å‡½æ•°
    """
    logger.info("ğŸ”§ LLaMA Factory é…ç½®æ–‡ä»¶ç”Ÿæˆå’Œè®­ç»ƒè„šæœ¬")
    logger.info("=" * 50)

    # 1. åˆ›å»ºYAMLé…ç½®æ–‡ä»¶
    logger.info("ğŸ“ æ­¥éª¤1: åˆ›å»ºè®­ç»ƒyamlé…ç½®æ–‡ä»¶...")
    config_path = create_train_yaml_config(dataset_dir=dataset_dir,
                                           dataset=dataset,
                                           output_dir=output_dir,
                                           model_name_or_path=model_name_or_path,
                                           yaml_file_path=yaml_file_path,
                                           train_max_samples=train_max_samples,
                                           num_train_epochs=num_train_epochs)  # create_train_yaml_config()

    if config_path is None:
        logger.info("âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    # æ˜¾ç¤ºåˆ›å»ºçš„é…ç½®æ–‡ä»¶å†…å®¹
    logger.info("\nğŸ“‹ ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å†…å®¹:")
    logger.info("-" * 30)
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            logger.info(f.read())
    except Exception as e:
        logger.info(f"æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")

    logger.info("-" * 30)

    # 2. æ‰§è¡Œè®­ç»ƒ
    logger.info("\nğŸš€ æ­¥éª¤2: æ‰§è¡Œè®­ç»ƒ...")
    success = run_llamafactory_train(config_path)

    if success:
        logger.info("\nğŸ‰ æ‰€æœ‰æ­¥éª¤å®Œæˆ!")
    else:
        logger.info("\nâš ï¸  é…ç½®æ–‡ä»¶å·²åˆ›å»ºï¼Œä½†è®­ç»ƒæ‰§è¡Œå¤±è´¥")
        logger.info(f"   æ‚¨å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œ: llamafactory-cli train {config_path}")


def run_llamafactory_predict(config_path):
    """
    ä½¿ç”¨llamafactory-cli trainè¿è¡Œè®­ç»ƒ
    """
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(config_path):
            logger.info(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False

        # æ„å»ºå‘½ä»¤
        cmd = ['llamafactory-cli', 'train', config_path]

        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¨ç†å‘½ä»¤: {' '.join(cmd)}")
        logger.info("=" * 50)

        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            capture_output=False,  # è®©è¾“å‡ºç›´æ¥æ˜¾ç¤ºåœ¨ç»ˆç«¯
            text=True,
            check=False  # ä¸è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸ï¼Œæ‰‹åŠ¨æ£€æŸ¥è¿”å›ç 
        )

        if result.returncode == 0:
            logger.info("=" * 50)
            logger.info("âœ… æ¨ç†å‘½ä»¤æ‰§è¡ŒæˆåŠŸ!")
            return True
        else:
            logger.info("=" * 50)
            logger.info(f"âŒ æ¨ç†å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            return False

    except FileNotFoundError:
        logger.info("âŒ æ‰¾ä¸åˆ° llamafactory-cli å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£… LLaMA-Factory")
        logger.info("   å¯ä»¥å°è¯•: pip install llamafactory")
        return False
    except Exception as e:
        logger.info(f"âŒ æ‰§è¡Œæ¨ç†æ—¶å‡ºé”™: {e}")
        return False


def llm_predict(dataset_dir, eval_dataset, output_dir, yaml_file_path, model_name_or_path,
                adapter_name_or_path=None, test_max_samples=100000, per_device_eval_batch_size=16, temperature=0.6):
    """
    ä¸»å‡½æ•°
    """
    logger.info("ğŸ”§ LLaMA Factory é…ç½®æ–‡ä»¶ç”Ÿæˆå’Œæ¨ç†è„šæœ¬")
    logger.info("=" * 50)

    # 1. åˆ›å»ºYAMLé…ç½®æ–‡ä»¶
    logger.info("ğŸ“ æ­¥éª¤1: åˆ›å»ºæ¨ç†yamlé…ç½®æ–‡ä»¶...")
    config_path = create_inference_yaml_config(dataset_dir=dataset_dir,
                                               eval_dataset=eval_dataset,
                                               output_dir=output_dir,
                                               model_name_or_path=model_name_or_path,
                                               adapter_name_or_path=adapter_name_or_path,
                                               yaml_file_path=yaml_file_path,
                                               test_max_samples=test_max_samples,
                                               per_device_eval_batch_size=per_device_eval_batch_size,
                                               temperature=temperature)

    if config_path is None:
        logger.info("âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    # æ˜¾ç¤ºåˆ›å»ºçš„é…ç½®æ–‡ä»¶å†…å®¹
    logger.info("\nğŸ“‹ ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å†…å®¹:")
    logger.info("-" * 30)
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            logger.info(f.read())
    except Exception as e:
        logger.info(f"æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")

    logger.info("-" * 30)

    # 2. æ‰§è¡Œæ¨ç†
    logger.info("\nğŸš€ æ­¥éª¤2: æ‰§è¡Œæ¨ç†...")
    success = run_llamafactory_predict(config_path)

    if success:
        logger.info("\nğŸ‰ æ‰€æœ‰æ­¥éª¤å®Œæˆ!")
    else:
        logger.info("\nâš ï¸  é…ç½®æ–‡ä»¶å·²åˆ›å»ºï¼Œä½†æ¨ç†æ‰§è¡Œå¤±è´¥")
        logger.info(f"   æ‚¨å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œ: llamafactory-cli train {config_path}")


if __name__ == "__main__":
    pass